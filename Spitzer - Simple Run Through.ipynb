{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spitzer Pipeline from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config IPython.matplotlib.backend = \"retina\"\n",
    "from matplotlib import rcParams\n",
    "rcParams[\"savefig.dpi\"] = 100\n",
    "rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "from celerite import plot_setup\n",
    "plot_setup.setup(auto=False)\n",
    "\n",
    "from pylab import *\n",
    "\n",
    "import batman\n",
    "import emcee3, corner\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from astropy.io import fits\n",
    "from glob import glob\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "emcee3.samplers.tqdm = tqdm_notebook\n",
    "\n",
    "from scipy.optimize import leastsq, minimize\n",
    "\n",
    "from scipy.signal import medfilt\n",
    "from scipy.stats import binned_statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Saved State**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwdSptzr  = '~/Research/Planets/WASP52/analysis/'\n",
    "saveDir   = 'SaveFiles/SaveState/' \n",
    "saveName  = 'save_state_r61517056_2457927.95867_FULL.pickle.save'\n",
    "# glob(saveDir + '*save_state*FULL*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pickle_dict = joblib.load(saveDir + saveName)# + 'FULL' + '.pickle.save')\n",
    "for key in load_pickle_dict.keys():\n",
    "    exec(key + \"= load_pickle_dict['\"+key+\"']\", globals(), locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**open the files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = '~/Research/Planets/WASP52/data/'\n",
    "\n",
    "AORNumber   = 'r61517056'\n",
    "channel     = 'ch1'\n",
    "fitsFileDir = 'raw/' + AORNumber + '/' + channel + '/bcd/'\n",
    "# fitsFileDir = 'raw/r51831552/ch1/bcd/'\n",
    "\n",
    "fitsfiles = glob(dataDir + fitsFileDir + '*bcd.fits')\n",
    "fitsfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfits    = fits.open(fitsfiles[0])[0]\n",
    "testheader  = testfits.header\n",
    "testheader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create function to loop over fits files and allocate necessary data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spitzer_load_fits_file(fitsFilenames, outputUnits='electrons'):\n",
    "    sec2day        = 1/(24*3600)\n",
    "    nFramesPerFile = 64\n",
    "    nSlopeFiles    = len(fitsFilenames)\n",
    "    testfits       = fits.open(fitsFilenames[0])[0]\n",
    "    testheader     = testfits.header\n",
    "    \n",
    "    bcd_shape      = testfits.data[0].shape\n",
    "    \n",
    "    nFrames        = nSlopeFiles * nFramesPerFile\n",
    "    imageCube      = np.zeros((nFrames, bcd_shape[0], bcd_shape[1]))\n",
    "    noiseCube      = np.zeros((nFrames, bcd_shape[0], bcd_shape[1]))\n",
    "    timeCube       = np.zeros(nFrames)\n",
    "    \n",
    "    del testfits\n",
    "    \n",
    "    # Converts DN/s to microJy per pixel\n",
    "    #   1) expTime * gain / fluxConv converts MJ/sr to electrons\n",
    "    #   2) as2sr * MJ2mJ * testheader['PXSCAL1'] * testheader['PXSCAL2'] converts MJ/sr to muJ/pixel\n",
    "    if outputUnits == 'electrons':\n",
    "        fluxConv  = testheader['FLUXCONV']\n",
    "        expTime   = testheader['EXPTIME']\n",
    "        gain      = testheader['GAIN']\n",
    "        fluxConversion = expTime*gain / fluxConv\n",
    "    elif outputUnits == 'muJ_per_Pixel':\n",
    "        as2sr = arcsec**2.0 # steradians per square arcsecond\n",
    "        MJ2mJ = 1e12        # mircro-Janskeys per Mega-Jansky\n",
    "        fluxConversion = abs(as2sr * MJ2mJ * testheader['PXSCAL1'] * testheader['PXSCAL2']) # converts MJ/\n",
    "    else:\n",
    "        raise Exception(\"`outputUnits` must be either 'electrons' or 'muJ_per_Pixel'\")\n",
    "    \n",
    "    for kfile, fname in tqdm_notebook(enumerate(fitsFilenames), \\\n",
    "                                      desc='Spitzer Load File', leave = False, total=nSlopeFiles):\n",
    "        bcdNow  = fits.open(fname)\n",
    "        # buncNow = fits.open(fname[:-len(filetype)] + 'bunc.fits')\n",
    "        buncNow = fits.open(fname.replace('bcd.fits', 'bunc.fits'))\n",
    "        \n",
    "        for iframe in range(nFramesPerFile):\n",
    "            timeCube[kfile * nFramesPerFile + iframe]  = bcdNow[0].header['BMJD_OBS'] \\\n",
    "                            + iframe * float(bcdNow[0].header['FRAMTIME']) * sec2day\n",
    "            \n",
    "            imageCube[kfile * nFramesPerFile + iframe] = bcdNow[0].data[iframe]  * fluxConversion\n",
    "            noiseCube[kfile * nFramesPerFile + iframe] = buncNow[0].data[iframe] * fluxConversion\n",
    "        \n",
    "        del bcdNow[0].data\n",
    "        bcdNow.close()\n",
    "        del bcdNow\n",
    "        \n",
    "        del buncNow[0].data\n",
    "        buncNow.close()\n",
    "        del buncNow\n",
    "    \n",
    "    return timeCube, imageCube, noiseCube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Call Spitzer Load Fits Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeCube, imageCube, noiseCube = spitzer_load_fits_file(fitsfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute Initial Background Estimates**\n",
    "\n",
    "This is necessary for our initial condition for centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nFrames           = timeCube.size\n",
    "median_background = median(imageCube, axis=(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examine the Initial Background Measurements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "istart, iskip = 64, 64\n",
    "plot(median_background, '.', alpha=0.02);\n",
    "istart, iskip = 0, 64\n",
    "plot(np.arange(median_background.size)[istart::iskip], median_background[istart::iskip], '.', alpha=0.2);\n",
    "istart, iskip = 63, 64\n",
    "plot(np.arange(median_background.size)[istart::iskip], median_background[istart::iskip], '.', alpha=0.2);\n",
    "plt.ylim(25,160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "istart, iskip = 64, 64\n",
    "for istart in range(64):\n",
    "    plot(np.arange(median_background.size)[istart::iskip], median_background[istart::iskip] - median(median_background[istart::iskip]), '.', alpha=0.2);\n",
    "# plot(np.arange(median_background.size)[0::64], median_background[0::64], '.', alpha=0.02);\n",
    "ylim(-5,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute Flux Weighted Centroid**\n",
    "\n",
    "This provides an initial condition for centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_flux_weighted_centroid(imageCube, yxguess, skybg, subSize=7):\n",
    "    '''\n",
    "        Flux-weighted centroiding (Knutson et al. 2008)\n",
    "        xpos and ypos are the rounded pixel positions of the star\n",
    "    '''\n",
    "    \n",
    "    y,x = 0,1\n",
    "    \n",
    "    ypos,xpos= yxguess\n",
    "    ## extract a box around the star:\n",
    "    \n",
    "    ylower   = int(ypos-subSize)\n",
    "    yupper   = int(ypos+subSize+1)\n",
    "    xlower   = int(xpos-subSize)\n",
    "    xupper   = int(xpos+subSize+1)\n",
    "    \n",
    "    nFrames  = imageCube.shape[0]\n",
    "    \n",
    "    flux_weighted_centroids = np.zeros((nFrames, 2))\n",
    "    \n",
    "    yrng  = arange(2*subSize+1)\n",
    "    xrng  = arange(2*subSize+1)\n",
    "    \n",
    "    for kf in tqdm_notebook(range(nFrames)):\n",
    "        subImage = imageCube[kf][ylower:yupper, xlower:xupper].copy()#.transpose()\n",
    "        \n",
    "        ## add up the flux along x and y\n",
    "        yflux = (subImage - skybg[kf]).sum(axis=y)\n",
    "        xflux = (subImage - skybg[kf]).sum(axis=x)\n",
    "        \n",
    "        ## get the flux weighted average position:\n",
    "        ypeak = sum(yflux * yrng) / sum(yflux) + (ypos - float(subSize))\n",
    "        xpeak = sum(xflux * xrng) / sum(xflux) + (xpos - float(subSize))\n",
    "        \n",
    "        flux_weighted_centroids[kf] = ypeak, xpeak\n",
    "    \n",
    "    return flux_weighted_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subSize = 7\n",
    "flux_weighted_centroids = compute_flux_weighted_centroid(imageCube, [15.,15.], median_background, subSize=subSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examine the Flux Weighted Centroids**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,x = 0,1\n",
    "plot(flux_weighted_centroids[:,y], '.', alpha=0.05);\n",
    "plot(flux_weighted_centroids[:,x], '.', alpha=0.05);\n",
    "ylim(14,16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute Gaussian Centroid**\n",
    "\n",
    "This provides an initial condition for centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian2D(params):\n",
    "    height, offset, ycenter, xcenter, ywidth, xwidth = params\n",
    "    return lambda yy,xx: height*exp(-0.5*(((ycenter-yy)/ywidth)**2. + ((xcenter-xx)/xwidth)**2.)) + offset\n",
    "\n",
    "def mle_neg_log_like(params, image, noise, yinds, xinds):\n",
    "    model = gaussian2D(params)(yinds, xinds)\n",
    "    return (((image - model) / noise)**2.).sum()\n",
    "\n",
    "def lsq_neg_log_like(params, image, noise, yinds, xinds):\n",
    "    model = gaussian2D(params)(yinds, xinds)\n",
    "    return (((image - model) / noise)**2.).ravel()\n",
    "\n",
    "def fit_gauss(image, noise, initParams, yinds, xinds, method='lsq'):\n",
    "    if method == 'lsq' or method == 'both':\n",
    "        soln = leastsq(lsq_neg_log_like, initParams, args=(image, noise, yinds, xinds))[0]\n",
    "    if method == 'mle':\n",
    "        soln = minimize(mle_neg_log_like, initParams, method=\"L-BFGS-B\", args=(image, noise, yinds, xinds))['x']\n",
    "    if method == 'both':\n",
    "        soln= minimize(mle_neg_log_like, soln, method=\"L-BFGS-B\", args=(image, noise, yinds, xinds))['x']\n",
    "    \n",
    "    return soln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_gaussian_centroids(imageCube, noiseCube, initCentroids, skybgs, subSize=7, method='lsq'):\n",
    "    \n",
    "    y,x = 0,1\n",
    "    \n",
    "    nFrames  = imageCube.shape[0]\n",
    "    \n",
    "    if np.linalg.matrix_rank(initCentroids) == 1:\n",
    "        ypos, xpos    = initCentroids\n",
    "        initCentroids = np.zeros((nFrames,2)) + initCentroids\n",
    "    \n",
    "    if np.linalg.matrix_rank(initCentroids) == 2:\n",
    "        ypos, xpos    = median(initCentroids,axis=0)\n",
    "    \n",
    "    yinds, xinds = np.indices((imageCube[0].shape))\n",
    "    \n",
    "    ypos     = np.int(np.round(ypos))\n",
    "    xpos     = np.int(np.round(xpos))\n",
    "    \n",
    "    ylower   = int(ypos-subSize)\n",
    "    yupper   = int(ypos+subSize+1)\n",
    "    xlower   = int(xpos-subSize)\n",
    "    xupper   = int(xpos+subSize+1)\n",
    "    \n",
    "    subYinds    = yinds[ylower:yupper, xlower:xupper]\n",
    "    subXinds    = xinds[ylower:yupper, xlower:xupper]\n",
    "    \n",
    "    gaussian_centroids = np.zeros((nFrames, 6))\n",
    "    \n",
    "    for kf in tqdm_notebook(range(nFrames)):\n",
    "        subImage    = imageCube[kf][ylower:yupper, xlower:xupper].copy()#.transpose()\n",
    "        subNoise    = noiseCube[kf][ylower:yupper, xlower:xupper].copy()#.transpose()\n",
    "        \n",
    "        initHeight  = subImage.max()\n",
    "        initOffset  = skybgs[kf]\n",
    "        initYcenter = initCentroids[kf][y]\n",
    "        initXcenter = initCentroids[kf][x]\n",
    "        initYwidth  = 0.85\n",
    "        initXwidth  = 0.85\n",
    "        initParams  = [initHeight, initOffset, initYcenter, initXcenter, initYwidth, initXwidth]\n",
    "        \n",
    "        gaussian_centroids[kf] = fit_gauss(subImage - skybgs[kf], subNoise, initParams, subYinds, subXinds, method)\n",
    "    \n",
    "    return gaussian_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_centroids_lsq = compute_gaussian_centroids(imageCube, noiseCube, [15.,15.], median_background, method='lsq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examine the Gaussian Centroids**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,x = 0,1\n",
    "plot(abs(gaussian_centroids_lsq.T[y+2]), '.', color='blue', alpha=1.0, label='y-centers')\n",
    "plot(abs(gaussian_centroids_lsq.T[x+2]), '.', color='orange', alpha=1.0, label='x-centers')\n",
    "legend(loc=0);\n",
    "# ylim(14.8, 15.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute Aperture Photometry**\n",
    "\n",
    "First by default, then with weight sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from photutils import CircularAperture, CircularAnnulus, EllipticalAperture\n",
    "from photutils import aperture_photometry\n",
    "\n",
    "def compute_photometry(image, noise, centroid, background, aperRad):\n",
    "    ypos, xpos  = centroid\n",
    "    flux_aper   = CircularAperture([xpos, ypos], aperRad)\n",
    "    \n",
    "    flux      = aperture_photometry(image - background, flux_aper         , \\\n",
    "                                    error=noise, mask=None       , \\\n",
    "                                    method='exact', subpixels=5 , \\\n",
    "                                    unit=None, wcs=None)['aperture_sum']\n",
    "    \n",
    "    return flux.data\n",
    "\n",
    "def compute_photometry_series(imageCube, noiseCube, centroids, backgrounds, aperRads):\n",
    "    nFrames = imageCube.shape[0]\n",
    "    \n",
    "    if np.rank(aperRads) == 0:\n",
    "        aperRads = aperRads*np.ones(nFrames)\n",
    "    \n",
    "    phots     = zeros(nFrames)\n",
    "    phots_err = zeros(nFrames)\n",
    "    \n",
    "    onesCube= ones(noiseCube.shape)\n",
    "    for kf in tqdm_notebook(range(nFrames)):\n",
    "        phots[kf]     = compute_photometry(imageCube[kf], \\\n",
    "                                           noiseCube[kf], \\\n",
    "                                           centroids.T[kf], \\\n",
    "                                           backgrounds[kf], \\\n",
    "                                           aperRads[kf])\n",
    "\n",
    "        phots_err[kf] = compute_photometry(noiseCube[kf], \\\n",
    "                                           onesCube[kf] , \\\n",
    "                                           centroids.T[kf], \\\n",
    "                                           backgrounds[kf], \\\n",
    "                                           aperRads[kf])\n",
    "\n",
    "    return phots, phots_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photmetry_2p5, photmetry_2p5_err = compute_photometry_series(imageCube, \\\n",
    "                                                             noiseCube, \\\n",
    "                                                             gaussian_centroids_lsq.T[2:4], \\\n",
    "                                                             median_background, 2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Effective PSF Width and Quadrature Widths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "npix     = 7\n",
    "midFrame = imageCube.shape[1]//2\n",
    "lower    = midFrame - npix\n",
    "upper    = midFrame + npix\n",
    "\n",
    "image_view        = imageCube[:,lower:upper,lower:upper]\n",
    "effective_widths  = image_view.sum(axis=(1,2))**2. / (image_view**2).sum(axis=(1,2))\n",
    "var_rads          = sqrt(effective_widths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_widths, x_widths= gaussian_centroids_lsq[:,4:].T\n",
    "quadrature_widths = sqrt(x_widths**2 + y_widths**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percLower, percMed, percUpper = np.percentile(var_rads, [50-68/2, 50, 50+68/2])\n",
    "hist(var_rads,bins=len(var_rads)//500, alpha=0.5);\n",
    "plt.axvline(percLower, ls='--', c='orange');\n",
    "plt.axvline(percMed  , ls='-' , c='orange');\n",
    "plt.axvline(percUpper, ls='--', c='orange');\n",
    "\n",
    "nSig = 5\n",
    "plt.xlim(percMed - nSig*(percMed-percLower), percMed + nSig*(percUpper - percMed));\n",
    "\n",
    "nSig = 3.3\n",
    "percLower, percMed, percUpper = np.percentile(nSig*quadrature_widths, [50-68/2, 50, 50+68/2])\n",
    "\n",
    "hist(nSig*quadrature_widths,bins=len(nSig*quadrature_widths)//500, alpha=0.5);\n",
    "plt.axvline(percLower, ls='--', c='violet');\n",
    "plt.axvline(percMed  , ls='-' , c='violet');\n",
    "plt.axvline(percUpper, ls='--', c='violet');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variable Aperture Photometry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photmetry_beta, photmetry_beta_err = compute_photometry_series(imageCube, \\\n",
    "                                                               noiseCube, \\\n",
    "                                                               gaussian_centroids_lsq.T[2:4], \\\n",
    "                                                               median_background, var_rads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nSig = 4.25\n",
    "photmetry_qwidth, photmetry_qwidth_err = compute_photometry_series(imageCube, \\\n",
    "                                                               noiseCube, \\\n",
    "                                                               gaussian_centroids_lsq.T[2:4], \\\n",
    "                                                               median_background, nSig*quadrature_widths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examine Binned Data for Correlation Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bin_array(arr, uncs = None,  binsize=100, KeepTheChange = False):\n",
    "    '''\n",
    "        Given nSize = size(time), nCols = binsize, nRows = nSize / nCols\n",
    "\n",
    "            this function reshapes the 1D array into a new 2D array of dimension\n",
    "                nCols x nRows or binsize x (nSize / nCols)\n",
    "\n",
    "            after which, the function collapses the array into a new 1D array by taking the median\n",
    "\n",
    "        Because most input arrays cannot be subdivided into an even number of subarrays of size `binsize`\n",
    "            we actually first slice the array into a 1D array of size `nRows*binsize`.\n",
    "\n",
    "            The mean of the remaining elements from the input array is then taken as the final element\n",
    "                in the output array\n",
    "\n",
    "    '''\n",
    "    nSize   = arr.size\n",
    "    nCols   = np.int(nSize / binsize)\n",
    "    nRows   = binsize\n",
    "\n",
    "    EqSize  = nRows*nCols\n",
    "    #LftOvers= nSize - EqSize\n",
    "\n",
    "    useArr  = arr[:EqSize].copy()   # this array can be subdivided evenly\n",
    "    #frstElt = np.median(arr[:LftOvers/2])   # mean of the remaining elements no able to be reshaped evenly\n",
    "    #lastElt = np.median(arr[-LftOvers:])   # mean of the remaining elements no able to be reshaped evenly\n",
    "\n",
    "    if uncs is not None:\n",
    "        # weighted mean profile\n",
    "        useUncs = uncs[:EqSize].copy()   # this array can be subdivided evenly\n",
    "        binArr  = np.median((useArr / useUncs).reshape(nCols, nRows).mean(axis=1)/ useUncs.reshape(nCols, nRows))\n",
    "        stdArr  = np.median((useArr / useUncs).reshape(nCols, nRows).std(axis=1) / useUncs.reshape(nCols, nRows))\n",
    "        \n",
    "        if KeepTheChange:\n",
    "            SpareArr    = arr[EqSize:].copy()\n",
    "            SpareUncs   = uncs[EqSize:].copy()\n",
    "\n",
    "            binTC       = np.median((SpareArr / SpareUncs)) / np.median(SpareUncs.reshape(nCols, nRows))\n",
    "            stdTC       = np.median((SpareArr / SpareUncs)) / np.median(SpareUncs.reshape(nCols, nRows))\n",
    "\n",
    "            binArr  = np.concatenate((binArr, [binTC]))\n",
    "            stdArr  = np.concatenate((stdArr, [stdTC]))\n",
    "    else:\n",
    "        # standard mean profile\n",
    "        binArr  = np.mean(useArr.reshape(nCols, nRows),axis=1)\n",
    "        stdArr  = np.std(useArr.reshape(nCols, nRows),axis=1) / sqrt(nSize)\n",
    "        \n",
    "        if KeepTheChange:\n",
    "            SpareArr    = arr[EqSize:].copy()\n",
    "            binTC       = np.median(SpareArr)\n",
    "            stdTC       = np.std(SpareArr)\n",
    "\n",
    "            binArr  = np.concatenate((binArr, [binTC]))\n",
    "            stdArr  = np.concatenate((stdArr, [stdTC]))\n",
    "\n",
    "    return binArr, stdArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nbins   = 120\n",
    "binsize = nFrames // nbins\n",
    "bin_flux, bin_flux_err = bin_array(photmetry_2p5, uncs = None,  binsize=binsize, KeepTheChange = False)\n",
    "bin_err, bin_err_err   = bin_array(photmetry_2p5_err, uncs = None,  binsize=binsize, KeepTheChange = False) \n",
    "bin_time, bin_time_err = bin_array(timeCube, uncs = None,  binsize=binsize, KeepTheChange = False)\n",
    "# bin_flux = binned_statistic(timeCube, photmetry_2p5[0], statistic='mean', bins=nbins, range=None)\n",
    "# bin_err  = binned_statistic(timeCube, photmetry_2p5_err[0], statistic='mean', bins=nbins, range=None)\n",
    "\n",
    "# timebin  = bin_flux.bin_edges[1:] - median(diff(bin_flux.bin_edges))\n",
    "# fluxbin  = bin_flux.statistic\n",
    "# errbin   = bin_err.statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(timeCube - timeCube.min(), photmetry_2p5, '.', alpha=1)\n",
    "errorbar(bin_time - timeCube.min(), bin_flux, bin_flux_err, fmt='o');\n",
    "ylim(39000,42000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcY          = gaussian_centroids_lsq.T[y+2].copy()\n",
    "gcX          = gaussian_centroids_lsq.T[x+2].copy()\n",
    "\n",
    "Ysort_inds   = np.argsort(gaussian_centroids_lsq.T[y+2])\n",
    "flux_Ysorted = photmetry_2p5[Ysort_inds]\n",
    "gcY_Ysorted  = gaussian_centroids_lsq.T[y+2][Ysort_inds]\n",
    "\n",
    "Xsort_inds   = np.argsort(gaussian_centroids_lsq.T[x+2])\n",
    "flux_Xsorted = photmetry_2p5[Xsort_inds]\n",
    "\n",
    "gcX_Xsorted  = gaussian_centroids_lsq.T[x+2][Xsort_inds]\n",
    "\n",
    "bin_fluxY, bin_fluxY_err = bin_array(flux_Ysorted, uncs = None,  binsize=binsize, KeepTheChange = False)\n",
    "bin_fluxX, bin_fluxX_err = bin_array(flux_Xsorted, uncs = None,  binsize=binsize, KeepTheChange = False)\n",
    "bin_gcY, bin_gcYerr    = bin_array(gcY_Ysorted, uncs = None,  binsize=binsize, KeepTheChange = False)\n",
    "bin_gcX, bin_gcXerr    = bin_array(gcX_Xsorted, uncs = None,  binsize=binsize, KeepTheChange = False)\n",
    "\n",
    "# plot(gaussian_centroids_lsq.T[2], photmetry_2p5[0], '.', alpha=0.2);\n",
    "plot(bin_gcY-(median(gaussian_centroids_lsq.T[y+2])), bin_fluxY,'o',alpha=0.5,mew=0);\n",
    "plot(bin_gcX-(median(gaussian_centroids_lsq.T[x+2])), bin_fluxX,'o',alpha=0.5,mew=0);\n",
    "# ylim(7700,8000);\n",
    "# xlim(14.9,15.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BATMAN Transit/Eclipse Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def b2inc(b, aRs, e = 0, w = 0):\n",
    "    #convert_b_to_inc\n",
    "    if e == 0:\n",
    "        return np.arccos(np.abs(b / aRs))\n",
    "    elif w == 0:\n",
    "        return np.arccos(np.abs(b / aRs / (1-e*e)))\n",
    "    else:\n",
    "        return np.arccos(np.abs(b / aRs / (1-e*e) * (1 - e*np.sin(w))))\n",
    "\n",
    "\n",
    "def deltaphase_eclipse(ecc, omega, omegatype = 'RV'):\n",
    "    from numpy import cos, pi, abs\n",
    "    return 0.5*( 1 + (4. / pi) * ecc * cos(omega))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2inc(88.99/180*pi, 14.64, 0.26493, (360-162.149)/180*pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h11Per       = 4.88782433\n",
    "h11t0        = 2454957.812464 - 2454833.0\n",
    "h11Inc       = 88.99\n",
    "h11ApRs      = 14.64\n",
    "h11RpRs      = 0.05856\n",
    "h11Ecc       = 0.26493\n",
    "h11Omega     = -162.149\n",
    "h11u1        = 0.646\n",
    "h11u2        = 0.048\n",
    "\n",
    "def batman_wrapper_raw(init_params, times, ldtype='quadratic', transitType='primary'):\n",
    "    \n",
    "    period, tcenter, inc, aprs, rprs, ecc, omega, u1, u2 = init_params\n",
    "    \n",
    "    bm_params           = batman.TransitParams() # object to store transit parameters\n",
    "    \n",
    "    bm_params.per       = period  # orbital period\n",
    "    bm_params.t0        = tcenter # time of inferior conjunction\n",
    "    bm_params.inc       = inc     # inclunaition in degrees\n",
    "    bm_params.a         = aprs    # semi-major axis (in units of stellar radii)\n",
    "    bm_params.rp        = rprs    # planet radius (in units of stellar radii)\n",
    "    bm_params.ecc       = ecc     # eccentricity\n",
    "    bm_params.w         = omega   # longitude of periastron (in degrees)\n",
    "    bm_params.limb_dark = ldtype              # limb darkening model # NEED TO FIX THIS\n",
    "    bm_params.u         = [u1, u2]                  # limb darkening coefficients # NEED TO FIX THIS\n",
    "    \n",
    "    m_eclipse = batman.TransitModel(bm_params, times, transittype=transittype)    # initializes model\n",
    "    \n",
    "    return m_eclipse\n",
    "\n",
    "def loglikehood(params, uni_prior, times, flux, fluxerr):\n",
    "    model = batman_wrapper_raw(params, times)\n",
    "    chisq = ((flux - model)/fluxerr)**2.\n",
    "    return -0.5*chisq.sum() # + lambda*abs(params).sum() # + lambda*np.sqrt((params**2).sum())\n",
    "\n",
    "def logPrior(params, uni_prior, times, flux, fluxerr):\n",
    "    for kp, lower, upper in enumerate(uni_prior):\n",
    "        if params[kp] < lower or params[k] > upper:\n",
    "            return -np.inf\n",
    "        return 0.0\n",
    "\n",
    "def logPosterior(params, uni_prior, times, flux, fluxerr):\n",
    "    logPriorNow = logPrior(params, uni_prior, times, flux, fluxerr)\n",
    "    logLikeLNow = loglikehood(params, uni_prior, times, flux, fluxerr)\n",
    "    return logLikeLNow + logPriorNow\n",
    "\n",
    "def neg_logprobability(params, uni_prior, times, flux, fluxerr):\n",
    "    return -2*logPosterior(params, uni_prior, times, flux, fluxerr)\n",
    "\n",
    "periodIn    = h11Per\n",
    "tcenterIn   = h11t0\n",
    "incIn       = h11Inc\n",
    "aprsIn      = h11ApRs\n",
    "rprsIn      = h11RpRs\n",
    "eccIn       = h11Ecc\n",
    "omegaIn     = h11Omega\n",
    "u1In        = h11u1\n",
    "u2In        = h11u2\n",
    "\n",
    "# Initial Parameters\n",
    "initParams = [periodIn, tcenterIn, incIn, aprsIn, rprsIn, eccIn, omegaIn, u1In, u2In]\n",
    "\n",
    "# Frozen Prior\n",
    "uniPrior = [\n",
    "            [periodIn,periodIn],\n",
    "            [tcenterIn, tcenterIn],\n",
    "            [incIn, incIn],\n",
    "            [aprsIn, aprsIn],\n",
    "            [rprsIn, rprsIn],\n",
    "            [eccIn,eccIn],\n",
    "            [omegaIn,omegaIn],\n",
    "            [u1In,u1In],\n",
    "            [u2In,u2In]\n",
    "           ]\n",
    "\n",
    "# Partial UnFrozen Prior\n",
    "uniPrior = np.array([\n",
    "            [periodIn,periodIn],\n",
    "            [tcenterIn-0.1, tcenterIn+0.1],\n",
    "            [80., 90.],\n",
    "            [10, 20],\n",
    "            [0.01, 0.1],\n",
    "            [eccIn,eccIn],\n",
    "            [omegaIn,omegaIn],\n",
    "            [0.6,0.7],\n",
    "            [0.0,0.1]\n",
    "           ])\n",
    "\n",
    "minimize(neg_logprobability, initParams, args=(uniPrior, timesSlice3, fluxSlice3, ferrSlice3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, (l,u) in enumerate(uniPrior):\n",
    "    print(k,l,u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batman_wrapper(params_in, times, ldtype = 'uniform', transittype=\"primary\"):\n",
    "    '''\n",
    "        INPUT:\n",
    "            params_in : 1D numpy array with parameters to be arranged into \n",
    "    '''\n",
    "    iPeriod, iTCenter, iBImpact, iRsAp, iEdepth, iTdepth, iEcc, iOmega  = range(len(params_in))\n",
    "    \n",
    "    bm_params           = batman.TransitParams() # object to store transit parameters\n",
    "    \n",
    "    bm_params.per       = params_in[iPeriod]  # orbital period\n",
    "    bm_params.t0        = params_in[iTCenter] # time of inferior conjunction\n",
    "    bm_params.bImpact   = params_in[iBImpact] # b, impact parameter\n",
    "    bm_params.r_a       = params_in[iRsAp]    # \n",
    "    bm_params.a         = 1.0 / bm_params.r_a # semi-major axis (in units of stellar radii)\n",
    "    bm_params.fp        = params_in[iEdepth]  # \n",
    "    bm_params.tdepth    = params_in[iTdepth]  # from Fraine et al. 2014s\n",
    "    bm_params.rp        = sqrt(params_in[iTdepth]) # planet radius (in units of stellar radii)\n",
    "    bm_params.ecc       = params_in[iEcc]     # eccentricity\n",
    "    bm_params.w         = params_in[iOmega]   # longitude of periastron (in degrees)\n",
    "    bm_params.inc       = b2inc(bm_params.bImpact, bm_params.a, bm_params.ecc, bm_params.w)*180/pi # orbital inclination (in degrees)\n",
    "    bm_params.limb_dark = ldtype              # limb darkening model # NEED TO FIX THIS\n",
    "    bm_params.u         = []                  # limb darkening coefficients # NEED TO FIX THIS\n",
    "    # print(bm_params.tdepth)\n",
    "    bm_params.delta_phase = deltaphase_eclipse(bm_params.ecc, bm_params.w)\n",
    "    bm_params.t_secondary = bm_params.t0 + bm_params.per*bm_params.delta_phase\n",
    "    \n",
    "    m_eclipse = batman.TransitModel(bm_params, times, transittype=transittype)    # initializes model\n",
    "    \n",
    "    return m_eclipse.light_curve(bm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exoparams import PlanetParams\n",
    "\n",
    "wasp52_par = PlanetParams('WASP-52 b')\n",
    "\n",
    "iPeriod   = wasp52_par.per.value\n",
    "iTCenter  = wasp52_par.t0.value-2400000.5\n",
    "iBImpact  = wasp52_par.b.value\n",
    "iRsAp     = 1.0/wasp52_par.ar.value\n",
    "iEdepth   = 100/1e6\n",
    "iTdepth   = wasp52_par.depth.value\n",
    "iEcc      = wasp52_par.ecc.value\n",
    "iOmega    = wasp52_par.om.value\n",
    "\n",
    "initParams_in = np.array([iPeriod, iTCenter, iBImpact, iRsAp, iEdepth, iTdepth, iEcc, iOmega])\n",
    "plot(timeCube, batman_wrapper(initParams_in, timeCube, transittype=\"primary\"));\n",
    "plot(timeCube, batman_wrapper(initParams_in, timeCube, transittype=\"secondary\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gausssian Kernel Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_nbr_qhull(xpos, ypos, npix, sm_num = 100, a = 1.0, b = 1.0, c = 1.0, print_space = 10000.):\n",
    "    '''\n",
    "        Python Implimentation of N. Lewis method, described in Lewis etal 2012, Knutson etal 2012\n",
    "\n",
    "        Taken from N. Lewis IDL code:\n",
    "\n",
    "            Construct a 3D surface (or 2D if only using x and y) from the data\n",
    "            using the qhull.pro routine.  Save the connectivity information for\n",
    "            each data point that was used to construct the Delaunay triangles (DT)\n",
    "            that form the grid.  The connectivity information allows us to only\n",
    "            deal with a sub set of data points in determining nearest neighbors\n",
    "            that are either directly connected to the point of interest or\n",
    "            connected through a neighboring point\n",
    "\n",
    "        Python Version:\n",
    "\n",
    "        J. Fraine    first edition, direct translation from IDL 12.05.12\n",
    "    '''\n",
    "    from scipy.spatial import cKDTree\n",
    "    #The surface fitting performs better if the data is scattered about zero\n",
    "\n",
    "    npix    = np.sqrt(npix)\n",
    "\n",
    "    x0  = (xpos - np.median(xpos))/a\n",
    "    y0  = (ypos - np.median(ypos))/b\n",
    "\n",
    "    if np.sum(npix) != 0.0 and c != 0:\n",
    "        np0 = (npix - np.median(npix))/c\n",
    "    else:\n",
    "        if np.sum(npix) == 0.0:\n",
    "            print('SKIPPING Noise Pixel Sections of Gaussian Kernel because Noise Pixels are Zero')\n",
    "        if c == 0:\n",
    "            print('SKIPPING Noise Pixel Sections of Gaussian Kernel because c == 0')\n",
    "\n",
    "    k            = sm_num                           # This is the number of nearest neighbors you want\n",
    "    n            = x0.size                          # This is the number of data points you have\n",
    "    nearest      = np.zeros((k,n),dtype=np.int64)   # This stores the nearest neighbors for each data point\n",
    "\n",
    "    #Multiplying by 1000.0 avoids precision problems\n",
    "    if npix.sum() != 0.0 and c != 0:\n",
    "        kdtree  = cKDTree(np.transpose((y0*1000., x0*1000., np0*1000.)))\n",
    "    else:\n",
    "        kdtree  = cKDTree(np.transpose((y0*1000., x0*1000.)))\n",
    "\n",
    "    gw  = np.zeros((k,n),dtype=np.float64) # This is the gaussian weight for each data point determined from the nearest neighbors\n",
    "    \n",
    "    for point in tqdm_notebook(range(n),total=n):\n",
    "        ind         = kdtree.query(kdtree.data[point],sm_num+1)[1][1:]\n",
    "        dx          = x0[ind] - x0[point]\n",
    "        dy          = y0[ind] - y0[point]\n",
    "\n",
    "        if npix.sum() != 0.0 and c != 0:\n",
    "            dnp         = np0[ind] - np0[point]\n",
    "\n",
    "        sigx        = np.std(dx )\n",
    "        sigy        = np.std(dy )\n",
    "        if npix.sum() != 0.0 and c != 0:\n",
    "            signp       = np.std(dnp)\n",
    "        if npix.sum() != 0.0 and c != 0:\n",
    "            gw_temp     = np.exp(-dx**2./(2.0*sigx**2.)) * \\\n",
    "                          np.exp(-dy**2./(2.*sigy**2.))  * \\\n",
    "                          np.exp(-dnp**2./(2.*signp**2.))\n",
    "        else:\n",
    "            gw_temp     = np.exp(-dx**2./(2.0*sigx**2.)) * \\\n",
    "                          np.exp(-dy**2./(2.*sigy**2.))\n",
    "\n",
    "        gw_sum      = gw_temp.sum()\n",
    "        gw_sum      = gw_temp.sum()\n",
    "        gw[:,point] = gw_temp/gw_sum\n",
    "\n",
    "        if (gw_sum == 0.0) or ~np.isfinite(gw_sum):\n",
    "            print('(gw_sum == 0.0) or ~isfinite(gw_temp))')\n",
    "            \n",
    "        nearest[:,point]  = ind\n",
    "\n",
    "    return gw.transpose(), nearest.transpose() # nearest  == nbr_ind.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.robust import scale\n",
    "def clipOutliers(array, nBins=101, nSig=10):\n",
    "        if not (nBins % 2):\n",
    "            print('nBins must be odd; `clipOutliers` is adding 1')\n",
    "            nBins += 1\n",
    "        \n",
    "        medfilt_array   = medfilt(array, nBins)\n",
    "        mad_array       = scale.mad(array)\n",
    "        outliers        = abs(array - medfilt_array) > nSig * mad_array\n",
    "        array[outliers] = medfilt_array[outliers]\n",
    "        \n",
    "        return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decorrelate_gk(times, phots, params, gk, nbr_ind, transittype=\"primary\"):\n",
    "    '''\n",
    "        Compute the Gaussian weighted fuction to decorelate photometry using the Gaussian kernel `gk`\n",
    "    '''\n",
    "    nbr_ind = np.array(nbr_ind, int).copy()\n",
    "    tmodel  = batman_wrapper(params, times, transittype=\"primary\")\n",
    "    return np.sum((phots / tmodel)[nbr_ind]*gk,axis=1)\n",
    "\n",
    "def decorrelate_gk_functionless(phots, gk, nbr_ind):\n",
    "    '''\n",
    "        form gaussian weighting fuction to decorelate photometry using a gaussian kernel `gk`\n",
    "    '''\n",
    "    nbr_ind = np.array(nbr_ind, int).copy()\n",
    "    return np.sum(phots[nbr_ind]*gk,axis=1)\n",
    "\n",
    "def gk_weighting_wrapper(gk_params):\n",
    "    times, flux, gk, nbr = gk_params\n",
    "    return lambda transit_params: decorrelate_gk(times, flux, transit_params, gk, nbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,x = 0,1\n",
    "xpos = clipOutliers(gaussian_centroids_lsq[:,x+2].copy())\n",
    "ypos = clipOutliers(gaussian_centroids_lsq[:,y+2].copy())\n",
    "npix1= clipOutliers(var_rads.copy())\n",
    "npix2= clipOutliers(4.25*quadrature_widths.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw1, nbr1 = find_nbr_qhull(xpos, ypos, npix1, sm_num=100, a=1.0, b=1.0, c=1.0)\n",
    "gw2, nbr2 = find_nbr_qhull(xpos, ypos, npix2, sm_num=100, a=1.0, b=1.0, c=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fix NaN Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if len(where(np.isnan(gw2))[0]):\n",
    "    ibad = where(np.isnan(gw2))\n",
    "    gw2[ibad] = 0.5*(gw2[ibad-1]+gw2[ibad+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute All Necessary Noise Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log(std(np.diff(clipOutliers(noise_model11) / median(clipOutliers(noise_model11))))*1e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noise_model11 = decorrelate_gk(timeCube, photmetry_2p5   , initParams_in, gw1, nbr1, transittype=\"primary\")\n",
    "noise_model12 = decorrelate_gk(timeCube, photmetry_beta  , initParams_in, gw1, nbr1, transittype=\"primary\")\n",
    "noise_model13 = decorrelate_gk(timeCube, photmetry_qwidth, initParams_in, gw1, nbr1, transittype=\"primary\")\n",
    "noise_model21 = decorrelate_gk(timeCube, photmetry_2p5   , initParams_in, gw2, nbr2, transittype=\"primary\")\n",
    "noise_model22 = decorrelate_gk(timeCube, photmetry_beta  , initParams_in, gw2, nbr2, transittype=\"primary\")\n",
    "noise_model23 = decorrelate_gk(timeCube, photmetry_qwidth, initParams_in, gw2, nbr2, transittype=\"primary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save State**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "save_pickle_dict = {'dataDir':dataDir,\\\n",
    "                    'fitsFileDir':fitsFileDir,\\\n",
    "                    'fitsfiles':fitsfiles,\\\n",
    "                    # 'testfits':testfits,\\\n",
    "                    'testheader':testheader,\\\n",
    "                    'timeCube':timeCube,\\\n",
    "                    'imageCube':imageCube,\\\n",
    "                    'noiseCube':noiseCube,\\\n",
    "                    'nFrames':nFrames,\\\n",
    "                    'median_background':median_background,\\\n",
    "                    'flux_weighted_centroids':flux_weighted_centroids,\\\n",
    "                    'gaussian_centroids_lsq':gaussian_centroids_lsq,\\\n",
    "                    'photmetry_2p5':photmetry_2p5,\\\n",
    "                    'photmetry_2p5_err':photmetry_2p5_err,\\\n",
    "                    'var_rads':var_rads,\\\n",
    "                    'quadrature_widths':quadrature_widths,\\\n",
    "                    'photmetry_beta':photmetry_beta,\\\n",
    "                    'photmetry_beta_err':photmetry_beta_err,\\\n",
    "                    'photmetry_qwidth':photmetry_qwidth,\\\n",
    "                    'photmetry_qwidth_err':photmetry_qwidth_err,\\\n",
    "                    'gw1':gw1,\\\n",
    "                    'nbr1':nbr1,\\\n",
    "                    'gw2':gw2,\\\n",
    "                    'nbr2':nbr2,\\\n",
    "                    'noise_model11':noise_model11,\\\n",
    "                    'noise_model12':noise_model12,\\\n",
    "                    'noise_model13':noise_model13,\\\n",
    "                    'noise_model21':noise_model21,\\\n",
    "                    'noise_model22':noise_model22,\\\n",
    "                    'noise_model23':noise_model23}\n",
    "\n",
    "# save_pickle_list = [dataDir, fitsFileDir, fitsfiles, testfits, testheader, timeCube, imageCube, noiseCube, \n",
    "#                     nFrames, median_background, flux_weighted_centroids, gaussian_centroids_lsq, photmetry_2p5, \n",
    "#                     photmetry_2p5_err, var_rads, quadrature_widths, photmetry_beta, photmetry_beta_err, \n",
    "#                     photmetry_qwidth, photmetry_qwidth_err, gw1, nbr1, gw2, nbr2, noise_model11, noise_model12, \n",
    "#                     noise_model13, noise_model21, noise_model22, noise_model23]\n",
    "\n",
    "import jdcal\n",
    "import datetime\n",
    "fmt = '%Y.%m.%d'\n",
    "dt = datetime.datetime.now()#strptime(s, fmt)\n",
    "tt = dt.timetuple()\n",
    "tt.tm_year * 1000 + tt.tm_yday\n",
    "jd_string = str(sum(jdcal.gcal2jd(dt.year, dt.month, dt.day)) + dt.hour/24. + dt.minute/24/3600)\n",
    "\n",
    "saveDir   = 'SaveFiles/SaveState/' \n",
    "saveName  = 'save_state_'+ AORNumber + '_' + jd_string + '_'#'_06.21.17_-_16:45_'\n",
    "print(saveDir + saveName)\n",
    "\n",
    "for thing in save_pickle_dict:\n",
    "    joblib.dump(save_pickle_dict[thing], saveDir + saveName + thing + '.pickle.save')\n",
    "\n",
    "joblib.dump(save_pickle_dict, saveDir + saveName + 'FULL' + '.pickle.save')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examine the Effect of Each Noise Model on the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeCube.min(), initParams_in[1]+0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(timeCube - timeCube.mean(), photmetry_2p5/median(photmetry_2p5),'.')\n",
    "xlim(-.03,.05)\n",
    "ylim(0.95,1.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramsTest = np.copy(initParams_in)\n",
    "plot(timeCube, photmetry_2p5/median(photmetry_2p5),'.')\n",
    "\n",
    "paramsTest[1] = tcenter_range[1]#initParams_in[1]+0.05\n",
    "noise_and_trans_model = gk_noise_and_transit_model(timeCube, photmetry_2p5, paramsTest, gw1, nbr1, transittypeIn=\"primary\")\n",
    "plot(timeCube, noise_and_trans_model / median(noise_and_trans_model),'.')\n",
    "\n",
    "paramsTest[1] = tcenter_range[0]#initParams_in[1]+0.05\n",
    "noise_and_trans_model = gk_noise_and_transit_model(timeCube, photmetry_2p5, paramsTest, gw1, nbr1, transittypeIn=\"primary\")\n",
    "plot(timeCube, noise_and_trans_model / median(noise_and_trans_model),'.')\n",
    "\n",
    "ylim(.9,1.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot(211)\n",
    "plot(timeCube,photmetry_2p5 / median(photmetry_2p5),'.');\n",
    "plot(timeCube,noise_model11 / median(photmetry_2p5),'.');\n",
    "plot(timeCube, noise_and_trans_model,'.');\n",
    "ylim(.9,1.1)\n",
    "subplot(212)\n",
    "plot(timeCube, photmetry_2p5 / noise_model11,'.');\n",
    "ylim(.9,1.1)\n",
    "subplot(212)\n",
    "print(std(photmetry_2p5 / median(photmetry_2p5)), std(noise_model11 / median(photmetry_2p5)), std(photmetry_2p5 / noise_model11))\n",
    "plot(timeCube, batman_wrapper(initParams_in, timeCube, transittype=\"primary\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot(211)\n",
    "plot(timeCube,photmetry_2p5 / median(photmetry_2p5),'.');\n",
    "plot(timeCube,noise_model11 / median(photmetry_2p5),'.')\n",
    "# ylim(.9,1.1)\n",
    "ylim(.98,1.02)\n",
    "plt.xlim(.39+5.7679e4,.4+5.7679e4)\n",
    "subplot(212)\n",
    "plot(timeCube, photmetry_2p5 / noise_model11,'.');\n",
    "ylim(.9,1.1)\n",
    "subplot(212)\n",
    "print(std(photmetry_2p5 / median(photmetry_2p5)), std(noise_model11 / median(photmetry_2p5)), std(photmetry_2p5 / noise_model11))\n",
    "plot(timeCube, batman_wrapper(initParams_in, timeCube, transittype=\"primary\"));\n",
    "ylim(.98,1.02)\n",
    "plt.xlim(.39+5.7679e4,.4+5.7679e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bin_fluxY, bin_fluxY_err = bin_array(photmetry_2p5 / noise_model11, uncs = None,  binsize=binsize, KeepTheChange = False)\n",
    "bin_time, bin_time_err   = bin_array(timeCube, uncs = None,  binsize=binsize, KeepTheChange = False)\n",
    "\n",
    "# plot(gaussian_centroids_lsq.T[2], photmetry_2p5[0], '.', alpha=0.2);\n",
    "# plot(bin_gcY-(median(gaussian_centroids_lsq.T[y+2])), bin_fluxY,'o',alpha=0.5,mew=0);\n",
    "# plot(bin_gcX-(median(gaussian_centroids_lsq.T[x+2])), bin_fluxX,'o',alpha=0.5,mew=0);\n",
    "# ylim(7700,8000);\n",
    "# xlim(14.9,15.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimzation with Negative Log Likelihood**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIGNUM        = 1e10\n",
    "\n",
    "period_range  = 0.0, BIGNUM\n",
    "tcenter_range = iTCenter - 0.05, iTCenter + 0.05\n",
    "bimpact_range = -1.0, 1.0\n",
    "RsAp_range    = 0.0, BIGNUM\n",
    "edepth_range  = 0.0, 1.0\n",
    "tdepth_range  = 0.0, 0.05\n",
    "ecc_range     = 0.0, 1.0\n",
    "omega_range   = 0.0, 360.\n",
    "\n",
    "params_dict = dict( Period   = dict(value=iPeriod  , fit=False, priorRange=period_range , priorType='uniform', index=0),\n",
    "                    TCenter  = dict(value=iTCenter , fit=True , priorRange=tcenter_range, priorType='uniform', index=1),\n",
    "                    BImpact  = dict(value=iBImpact , fit=False, priorRange=bimpact_range, priorType='uniform', index=2),\n",
    "                    RsAp     = dict(value=iRsAp    , fit=False, priorRange=RsAp_range   , priorType='uniform', index=3),\n",
    "                    Edepth   = dict(value=iEdepth  , fit=False , priorRange=edepth_range , priorType='uniform', index=4),\n",
    "                    Tdepth   = dict(value=iTdepth  , fit=True , priorRange=tdepth_range , priorType='uniform', index=5),\n",
    "                    Ecc      = dict(value=iEcc     , fit=False, priorRange=ecc_range    , priorType='uniform', index=6),\n",
    "                    Omega    = dict(value=iOmega   , fit=False, priorRange=omega_range  , priorType='uniform', index=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "DataFrame(params_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gk_noise_and_transit_model(timesIn, photsIn, paramsIn, gkIn, nbr_indIn, transittypeIn=\"primary\"):\n",
    "    '''\n",
    "        Compute the Gaussian weighted fuction to decorelate photometry using the Gaussian kernel `gk`\n",
    "    '''\n",
    "    nbr_ind = np.array(nbr_indIn, int).copy()\n",
    "    tmodel  = batman_wrapper(paramsIn, timesIn, transittype=transittypeIn)\n",
    "    return (np.sum((photsIn - tmodel)[nbr_ind]*gkIn, axis=1)+1)*tmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_prior_function(paramsIn, paramDictIn, timesIn, photsIn, phots_errIn, gwIn, nbrIn, transittypeIn, minTypeIn):\n",
    "    sqrt2pi = sqrt(2*pi)\n",
    "    logPrior  = 0.0\n",
    "    # print(minTypeIn, paramsIn)\n",
    "    paramKeys = list(paramDictIn.keys())\n",
    "    \n",
    "    idx       = 0\n",
    "    paramsUse = np.zeros(len(paramKeys))\n",
    "    for key in paramKeys:\n",
    "        paramIndex = np.copy(paramDictIn[key]['index'])\n",
    "        if not paramDictIn[key]['fit']:\n",
    "            paramsUse[paramIndex] = np.copy(paramDictIn[key]['value'])\n",
    "        else:\n",
    "            paramsUse[paramIndex] = np.copy(paramsIn[idx])\n",
    "            idx                  += 1\n",
    "    \n",
    "    for key in paramKeys:\n",
    "        paramNow = paramsUse[paramDictIn[key]['index']]\n",
    "        if paramDictIn[key]['priorType'] == 'uniform':\n",
    "            lPrior, uPrior  = paramDictIn[key]['priorRange']\n",
    "            isGreater = paramNow >= lPrior\n",
    "            isLower   = paramNow <= uPrior\n",
    "            \n",
    "            if isGreater and isLower:\n",
    "                pass\n",
    "            else:\n",
    "                if not paramDictIn[key]['fit']:\n",
    "                    raise Exception('Parameter ' + key + ' has value '  + str(paramNow)     +\\\n",
    "                                    ', but is out of the Prior Range [' + str(lPrior) + ', '+\\\n",
    "                                    str(uPrior) + '] && not set to fit')\n",
    "                return -np.inf\n",
    "        \n",
    "        if paramDictIn[key]['priorType'] == 'normal':\n",
    "            cPrior, sPrior  = paramDictIn[key]['priorRange']\n",
    "            logPrior += -0.5((paramNow - cPrior)/sPrior)**2. - ln(sqrt2pi*sPrior)\n",
    "        \n",
    "        if paramDictIn[key]['priorType'] == 'truncated_normal':\n",
    "            cPrior, sPrior, nSigPrior = paramDictIn[key]['priorRange']\n",
    "            if abs((cPrior - parmamNow) <= nSigPrior*sPrior):\n",
    "                logPrior += -0.5((paramNow - cPrior)/sPrior)**2. - ln(sqrt2pi*sPrior)\n",
    "            else:\n",
    "                return -np.inf\n",
    "    \n",
    "    return logPrior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neg_log_likelihood_function(paramsIn, paramDictIn, timesIn, photsIn, phots_errIn, gwIn, nbrIn, transittypeIn, minTypeIn):\n",
    "    \n",
    "    sqrt2pi = sqrt(2*pi)\n",
    "    \n",
    "    paramKeys = paramKeys = list(paramDictIn.keys())\n",
    "    # print(minTypeIn, paramsIn)\n",
    "    idx       = 0\n",
    "    paramsUse = np.zeros(len(paramKeys))\n",
    "    for key in paramKeys:\n",
    "        paramIndex = np.copy(paramDictIn[key]['index'])\n",
    "        if not paramDictIn[key]['fit']:\n",
    "            paramsUse[paramIndex] = np.copy(paramDictIn[key]['value'])\n",
    "        else:\n",
    "            # print(idx, paramDict[key]['index'], paramsIn, paramsUse)\n",
    "            paramsUse[paramIndex] = np.copy(paramsIn[idx])\n",
    "            idx                  += 1\n",
    "    # print(paramsUse[[1,5]])\n",
    "    model = gk_noise_and_transit_model(timesIn, photsIn, paramsUse, gwIn, nbrIn, transittypeIn=transittypeIn)\n",
    "    log_likelihood = -0.5*((photsIn - model)/phots_errIn)**2. # - log(sqrt(2*pi*phots_err))\n",
    "    \n",
    "    if minTypeIn == 'mle':\n",
    "        # log_prior = log_prior_function(paramsUse, paramDictIn, timesIn, photsIn, phots_errIn, \\\n",
    "        #                                gwIn, nbrIn, transittypeIn, minTypeIn)\n",
    "        # if not np.isfinite(log_prior):\n",
    "        #     return -np.inf\n",
    "        \n",
    "        return -2*log_likelihood.sum()\n",
    "    \n",
    "    if minTypeIn == 'lsq':\n",
    "        # log_prior = log_prior_function(paramsUse, paramDictIn, timesIn, photsIn, phots_errIn, \\\n",
    "        #                                gwIn, nbrIn, transittypeIn, minTypeIn)\n",
    "        # if not np.isfinite(log_prior):\n",
    "        #     return -np.inf*np.ones(log_likelihood.size)\n",
    "        \n",
    "        return -2*log_likelihood\n",
    "    \n",
    "    if minTypeIn == 'emcee':\n",
    "        # print(log_likelihood)\n",
    "        return log_likelihood.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize, leastsq\n",
    "\n",
    "times       = timeCube.copy()\n",
    "phots       = clipOutliers(photmetry_2p5.copy())\n",
    "phots_err   = clipOutliers(photmetry_2p5_err.copy())\n",
    "gw, nbr     = gw1.copy(), nbr1.copy()\n",
    "occulttype  = 'primary' \n",
    "minType     = 'emcee'\n",
    "\n",
    "args_mle = params_dict, times, phots, phots_err, gw, nbr, 'primary', 'mle'\n",
    "args_lsq = params_dict, times, phots, phots_err, gw, nbr, 'primary', 'lsq'\n",
    "\n",
    "pMLE = minimize(neg_log_likelihood_function, initParams_in[[1,5]], method='L-BFGS-B', args=args_mle)#, **kwargs=kwargs_mle)\n",
    "pLSQ = leastsq(neg_log_likelihood_function, initParams_in[[1,5]], args=args_lsq)#, **kwargs=kwargs_lsq)\n",
    "\n",
    "pMLE, pLSQ, initParams_in[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "times       = timeCube.copy()\n",
    "phots       = clipOutliers(photmetry_2p5.copy(), nSig=8)\n",
    "phots_err   = clipOutliers(photmetry_2p5_err.copy(), nSig=8)\n",
    "gw, nbr     = gw1.copy(), nbr1.copy()\n",
    "occulttype  = 'primary' \n",
    "minType     = 'emcee'\n",
    "\n",
    "model     = emcee3.SimpleModel(neg_log_likelihood_function, log_prior_function, \\\n",
    "    args=(params_dict, times, phots, phots_err, gw1, nbr1, occulttype, minType))\n",
    "\n",
    "freeParams = []\n",
    "for key in params_dict.keys():\n",
    "    if params_dict[key]['fit']:\n",
    "        freeParams.append(params_dict[key]['index'])\n",
    "\n",
    "print(freeParams)\n",
    "\n",
    "ndim, nwalkers = len(freeParams), 100\n",
    "coords = np.zeros((ndim,nwalkers))\n",
    "\n",
    "idx = 0\n",
    "for key in params_dict.keys():\n",
    "    if params_dict[key]['fit']:\n",
    "        initPNow = params_dict[key]['value']\n",
    "        initRng  = 0.05*params_dict[key]['value']\n",
    "        lPrior   = initPNow - initRng\n",
    "        uPrior   = initPNow + initRng\n",
    "        coords[idx]    = np.random.uniform(lPrior, uPrior, nwalkers)\n",
    "        idx           += 1\n",
    "\n",
    "# print(coords.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(coords=initParams_in[[1,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ensemble = emcee3.Ensemble(model, coords.T)\n",
    "\n",
    "print(\"The first walker has the coordinates: {0}\".format(ensemble[0].coords))\n",
    "print(\"and log prior={0:.3f}, log likelihood={1:.3f}, and log probability={2:.3f}\"\n",
    "      .format(ensemble[0].log_prior, ensemble[0].log_likelihood, ensemble[0].log_probability))\n",
    "\n",
    "nIters  = 100\n",
    "sampler = emcee3.Sampler()\n",
    "sampler.run(ensemble, nIters, progress=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "paramLabels = [\"$Period$\", \"$tCenter$\", r\"$b_{impact}$\", r\"$\\frac{Rs}{Ap}$\", \"$E-depth$\", \"$T-depth$\", \"$e$\", \"$\\omega$\"]\n",
    "\n",
    "# chain = sampler.get_coords(discard=10, flat=True) # need to change indicies in plot function\n",
    "chain = sampler.get_coords()\n",
    "fig, axes = plt.subplots(len(freeParams), 1, sharex=True, figsize=(6, 6))\n",
    "for k, nm in enumerate(np.array(paramLabels)[freeParams]):\n",
    "    if len(freeParams) > 1:\n",
    "        axes[k].plot(chain[:, :, k], alpha=0.3)\n",
    "#         axes[k].locator_params(tight=True, nbins=6)\n",
    "        axes[k].yaxis.set_label_coords(-0.15, 0.5)\n",
    "        axes[k].set_ylabel(nm)\n",
    "    else:\n",
    "        axes.plot(chain[:, :, k], alpha=0.3)\n",
    "        # axes.locator_params(tight=True, nbins=6)\n",
    "        axes.yaxis.set_label_coords(-0.15, 0.5)\n",
    "        axes.set_ylabel(nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "\n",
    "flatchain    = sampler.get_coords(discard=60, flat=True)\n",
    "medianParams = median(flatchain,axis=0)\n",
    "madParams    = scale.mad(flatchain,axis=0)\n",
    "stdParams    = std(flatchain,axis=0)\n",
    "\n",
    "print(medianParams*1e6, madParams*1e6, stdParams*1e6)\n",
    "\n",
    "corner.corner(flatchain[(flatchain>0.02004)*(flatchain<0.02005)], labels=np.array(paramLabels)[freeParams], truths=medianParams);\n",
    "# xlim(0.0200,0.201)\n",
    "fig = gcf()\n",
    "ax  = fig.get_axes()[0]\n",
    "# ax.set_xlim(.02,.0201)\n",
    "axvline(0.02)\n",
    "axvline(0.0201);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx       = 0\n",
    "paramsUse = np.zeros(len(params_dict.keys()))\n",
    "for key in params_dict.keys():\n",
    "    if not params_dict[key]['fit']:\n",
    "        paramIndex            = params_dict[key]['index']\n",
    "        paramsUse[paramIndex] = params_dict[key]['value']\n",
    "    else:\n",
    "        paramsUse[paramIndex] = medianParams\n",
    "        idx                  += 1\n",
    "\n",
    "transittype='primary'\n",
    "# for i in np.random.randint(len(flatchain), size=50):\n",
    "#     #for fp in freeParams:\n",
    "#     paramsUse[idx] = flatchain[i]\n",
    "#     model = gk_noise_and_transit_model(times, phots, paramsUse, gw, nbr, transittype=transittype)\n",
    "#     plt.plot(timeCube, model, \"orange\", alpha=0.1)\n",
    "\n",
    "# plot the data\n",
    "nbins   = 120\n",
    "binsize = nFrames // nbins\n",
    "\n",
    "times_bin, times_bin_err = bin_array(times        , uncs = None,  binsize=binsize, KeepTheChange = False)\n",
    "phots_bin, phots_bin_err = bin_array(phots        , uncs = None,  binsize=binsize, KeepTheChange = False)\n",
    "phots_err_bin, phots_err_err = bin_array(phots_err, uncs = None,  binsize=binsize, KeepTheChange = False)\n",
    "\n",
    "# plt.plot(times, phots, phots_err, 'k.', alpha=0.01)\n",
    "plt.errorbar(times_bin, phots_bin, phots_err_bin, fmt='.',color='k')\n",
    "\n",
    "\n",
    "paramsUse[idx] = medianParams\n",
    "modelMed = gk_noise_and_transit_model(times, phots, paramsUse, gw, nbr, transittype=transittype)\n",
    "modelMed_bin, modelMed__err = bin_array(modelMed, uncs = None,  binsize=binsize, KeepTheChange = False)\n",
    "\n",
    "plt.plot(times_bin, modelMed_bin, 'o',color=\"orange\", alpha=1)\n",
    "plt.ylim(8000,8500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = np.percentile(flatchain, [16, 50, 84], axis=0)\n",
    "mean = q[1]\n",
    "uncert = np.diff(q, axis=0).T\n",
    "\n",
    "for i, (nm, truth) in enumerate(zip(paramLabels,medianParams)):\n",
    "    display(Math(r\"{0}_\\mathrm{{MCMC}} = {1:.3f} _{{-{2:.3f}}}^{{+{3:.3f}}} \\quad (\\mathrm{{truth:}}\\,{4:.3f})\"\n",
    "                 .format(nm, mean[i], uncert[i][0], uncert[i][1], truth)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
